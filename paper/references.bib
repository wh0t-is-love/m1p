@inproceedings{gpt3,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{llama3,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and et al.},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@article{gpt4,
  added-at = {2023-10-23T16:08:31.000+0200},
  author = {OpenAI},
  biburl = {https://www.bibsonomy.org/bibtex/2b87062f1a9478148d2e5dd0006c9c455/tomvoelker},
  description = {Technical report detailing the development of GPT-4, a multimodal model capable of handling both image and text inputs. The model achieved human-level performance on various benchmarks, including scoring in the top 10% on a simulated bar exam. The study highlights the importance of the post-training alignment process for enhancing the model's accuracy and behavior.},
  interhash = {241e35649065841f159e6105eb87b1d3},
  intrahash = {b87062f1a9478148d2e5dd0006c9c455},
  journal = {ArXiv},
  keywords = {gpt-4 openai transformer multimodal bar_exam alignment_process paper_demo posted_with_chatgpt},
  timestamp = {2023-10-23T16:08:31.000+0200},
  title = {GPT-4 Technical Report},
  url = {https://arxiv.org/abs/2303.08774},
  volume = {abs/2303.08774},
  year = 2023
}

@inproceedings{learningTB,
 author = {Xu, Jin and Liu, Xiaojiang and Yan, Jianhao and Cai, Deng and Li, Huayang and Li, Jian},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {3082--3095},
 publisher = {Curran Associates, Inc.},
 title = {Learning to Break the Loop: Analyzing and Mitigating Repetitions for Neural Text Generation },
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/148c0aeea1c5da82f4fa86a09d4190da-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{nucleus,
title={The Curious Case of Neural Text Degeneration},
author={Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rygGQyrFvH}
}

@misc{sdxl,
      title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis}, 
      author={Dustin Podell and Zion English and Kyle Lacey and Andreas Blattmann and Tim Dockhorn and Jonas Müller and Joe Penna and Robin Rombach},
      year={2023},
      eprint={2307.01952},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.01952}, 
}

@inproceedings{latentdiffusion,
  added-at = {2022-09-05T11:03:26.000+0200},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  biburl = {https://www.bibsonomy.org/bibtex/28253f81df661643c915d38d2e317d17d/tobias.koopmann},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  interhash = {e54e035bdfef24c40a2133cbe99ac3bb},
  intrahash = {8253f81df661643c915d38d2e317d17d},
  keywords = {readinglist},
  pages = {10684--10695},
  timestamp = {2022-09-05T11:03:26.000+0200},
  title = {High-resolution image synthesis with latent diffusion models},
  year = 2022
}

@inproceedings{dm_distil,
    author    = {Meng, Chenlin and Rombach, Robin and Gao, Ruiqi and Kingma, Diederik and Ermon, Stefano and Ho, Jonathan and Salimans, Tim},
    title     = {On Distillation of Guided Diffusion Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {14297-14306}
}

@inproceedings{transformer,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@inproceedings{difformer,
    title = "Empowering Diffusion Models on the Embedding Space for Text Generation",
    author = "Gao, Zhujin  and
      Guo, Junliang  and
      Tan, Xu  and
      Zhu, Yongxin  and
      Zhang, Fang  and
      Bian, Jiang  and
      Xu, Linli",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.261",
    doi = "10.18653/v1/2024.naacl-long.261",
    pages = "4664--4683",
    abstract = "Diffusion models have achieved state-of-the-art synthesis quality on both visual and audio tasks, and recent works further adapt them to textual data by diffusing on the embedding space. In this paper, we conduct systematic studies of the optimization challenges encountered with both the embedding space and the denoising model, which have not been carefully explored. Firstly, the data distribution is learnable for embeddings, which may lead to the collapse of the embedding space and unstable training. To alleviate this problem, we propose a new objective called the anchor loss which is more efficient than previous methods. Secondly, we find the noise levels of conventional schedules are insufficient for training a desirable denoising model while introducing varying degrees of degeneration in consequence. To address this challenge, we propose a novel framework called noise rescaling. Based on the above analysis, we propose Difformer, an embedding diffusion model based on Transformer. Experiments on varieties of seminal text generation tasks show the effectiveness of the proposed methods and the superiority of Difformer over previous state-of-the-art embedding diffusion baselines.",
}

@inproceedings{diffusion-lm,
 author = {Li, Xiang and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S and Hashimoto, Tatsunori B},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {4328--4343},
 publisher = {Curran Associates, Inc.},
 title = {Diffusion-LM Improves Controllable Text Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/1be5bc25d50895ee656b8c2d9eb89d6a-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}

@misc{self-cond,
      title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning}, 
      author={Ting Chen and Ruixiang Zhang and Geoffrey Hinton},
      year={2023},
      eprint={2208.04202},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{ld4lg,
 author = {Lovelace, Justin and Kishore, Varsha and Wan, Chao and Shekhtman, Eliot and Weinberger, Kilian Q},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {56998--57025},
 publisher = {Curran Associates, Inc.},
 title = {Latent Diffusion for Language Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/b2a2bd5d5051ff6af52e1ef60aefd255-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}


@misc{sed,
      title={Self-conditioned Embedding Diffusion for Text Generation}, 
      author={Robin Strudel and Corentin Tallec and Florent Altché and Yilun Du and Yaroslav Ganin and Arthur Mensch and Will Grathwohl and Nikolay Savinov and Sander Dieleman and Laurent Sifre and Rémi Leblond},
      year={2022},
      eprint={2211.04236},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{score-based,
  author       = {Yang Song and
                  Jascha Sohl{-}Dickstein and
                  Diederik P. Kingma and
                  Abhishek Kumar and
                  Stefano Ermon and
                  Ben Poole},
  title        = {Score-Based Generative Modeling through Stochastic Differential Equations},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=PxTIG12RRHS},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/0011SKKEP21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{cfg,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}

@inproceedings{rocstories,
    title = "A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories",
    author = "Mostafazadeh, Nasrin  and
      Chambers, Nathanael  and
      He, Xiaodong  and
      Parikh, Devi  and
      Batra, Dhruv  and
      Vanderwende, Lucy  and
      Kohli, Pushmeet  and
      Allen, James",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1098",
    doi = "10.18653/v1/N16-1098",
    pages = "839--849",
}

@inproceedings{qqp,
  title={Quora Question Pairs},
  author={Zihang Chen and Hongbo Zhang and Xiaoji Zhang and Leqi Zhao},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:233225749}
}

@inproceedings{xsum,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
    editor = "Riloff, Ellen  and
      Chiang, David  and
      Hockenmaier, Julia  and
      Tsujii, Jun{'}ichi",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-1206",
    doi = "10.18653/v1/D18-1206",
    pages = "1797--1807",
    abstract = "We introduce {``}extreme summarization{''}, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question {``}What is the article about?{''}. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article{'}s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.",
}

@inproceedings{diffuseq,
title={DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models},
author={Shansan Gong and Mukai Li and Jiangtao Feng and Zhiyong Wu and Lingpeng Kong},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=jQj-_rLVXsj}
}

@inproceedings{mauve,
 author = {Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {4816--4828},
 publisher = {Curran Associates, Inc.},
 title = {MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/260c2432a0eecc28ce03c10dadc078a4-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{
diversity,
title={A Contrastive Framework for Neural Text Generation},
author={Yixuan Su and Tian Lan and Yan Wang and Dani Yogatama and Lingpeng Kong and Nigel Collier},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=V88BafmH9Pj}
}

@inproceedings{
ar-diffusion,
title={{AR}-Diffusion: Auto-Regressive Diffusion Model for Text Generation},
author={Tong Wu and Zhihao Fan and Xiao Liu and Hai-Tao Zheng and Yeyun Gong and yelong shen and Jian Jiao and Juntao Li and zhongyu wei and Jian Guo and Nan Duan and Weizhu Chen},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=0EG6qUQ4xE}
}

@inproceedings{genie,
author = {Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Duan, Nan and Chen, Weizhu},
title = {Text generation with diffusion language models: a pre-training approach with continuous paragraph denoise},
year = {2023},
publisher = {JMLR.org},
abstract = {In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pre-trained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSUM, CNN/DAILYMAIL, GIGAWORD, and COMMONGEN. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at https://github.com/microsoft/ProphetNet/tree/master/GENIE.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {867},
numpages = {14},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}

@inproceedings{planner,
title={{PLANNER}: Generating Diversified Paragraph via Latent Language Diffusion Model},
author={Yizhe Zhang and Jiatao Gu and Zhuofeng Wu and Shuangfei Zhai and Joshua M. Susskind and Navdeep Jaitly},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=SLwy8UVS8Y}
}

@misc{dinoiser,
      title={DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises}, 
      author={Jiasheng Ye and Zaixiang Zheng and Yu Bao and Lihua Qian and Mingxuan Wang},
      year={2024},
      eprint={2302.10025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.10025}, 
}

@inproceedings{bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@inproceedings{simple_diffusion,
author = {Hoogeboom, Emiel and Heek, Jonathan and Salimans, Tim},
title = {Simple diffusion: end-to-end diffusion for high resolution images},
year = {2023},
publisher = {OpenReview.net},
abstract = {Currently, applying diffusion models in pixel space of high resolution images is difficult. Instead, existing approaches focus on diffusion in lower dimensional spaces (latent diffusion), or have multiple super-resolution levels of generation referred to as cascades. The downside is that these approaches add additional complexity to the diffusion framework.This paper aims to improve denoising diffusion for high resolution images while keeping the model as simple as possible. The paper is centered around the research question: How can one train standard diffusion models on high resolution images, and still obtain performance comparable to these alternate approaches?The four main findings are: 1) the noise schedule should be adjusted for high resolution images, 2) It is sufficient to scale only a particular part of the architecture, 3) dropout should be added at specific locations in the architecture, and 4) downsampling is an effective strategy to avoid high resolution feature maps. Combining these simple yet effective techniques, we achieve state-of-the-art on image generation among diffusion models without sampling modifiers on ImageNet.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {537},
numpages = {20},
location = {Honolulu, Hawaii, USA},
series = {ICML'23}
}
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}
@article{li2022diffusion,
  title={Diffusion-lm improves controllable text generation},
  author={Li, Xiang and Thickstun, John and Gulrajani, Ishaan and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4328--4343},
  year={2022}
}
@inproceedings{lin2023text,
  title={Text generation with diffusion language models: A pre-training approach with continuous paragraph denoise},
  author={Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Duan, Nan and Chen, Weizhu},
  booktitle={International Conference on Machine Learning},
  pages={21051--21064},
  year={2023},
  organization={PMLR}
}
@inproceedings{han2022ssd,
    title = "{SSD}-{LM}: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control",
    author = "Han, Xiaochuang  and
      Kumar, Sachin  and
      Tsvetkov, Yulia",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.647",
    doi = "10.18653/v1/2023.acl-long.647",
    pages = "11575--11596",
    abstract = "Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM{---}a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, SSD-LM also outperforms competitive baselines, with an extra advantage in modularity.",
}

@inproceedings{yuan2022seqdiffuseq,
    title = "Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation",
    author = "Yuan, Hongyi  and
      Yuan, Zheng  and
      Tan, Chuanqi  and
      Huang, Fei  and
      Huang, Songfang",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.2",
    doi = "10.18653/v1/2024.naacl-long.2",
    pages = "22--39",
    abstract = "The diffusion model, a new generative modeling paradigm, has achieved great success in image, audio, and video generation.However, considering the discrete categorical nature of the text, it is not trivial to extend continuous diffusion models to natural language. In this work, we propose SeqDiffuSeq, a text diffusion model, to approach sequence-to-sequence text generation with an encoder-decoder Transformer architecture.To improve the generation performance, SeqDiffuSeq is equipped with the self-conditioning technique and our newly proposed adaptive noise schedule technique. Self-conditioning enables SeqDiffuSeq to better use the predicted sequence information during the generation process.The adaptive noise schedule balances the difficulty of denoising across time steps at the token level.Experiment results illustrate the improved performance on five sequence-to-sequence generation tasks compared to other diffusion-based models regarding text quality and inference time.",
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@misc{zhang2019bertscore,
      title={BERTScore: Evaluating Text Generation with BERT}, 
      author={Tianyi Zhang and Varsha Kishore and Felix Wu and Kilian Q. Weinberger and Yoav Artzi},
      year={2020},
      eprint={1904.09675},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09675}, 
}

@misc{gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019},
  howpublished="\url{https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}",
  note="OpenAI blog, p. 9"
}
@inproceedings{multinomial_diffusion,
 author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr\'{e}, Patrick and Welling, Max},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {12454--12465},
 publisher = {Curran Associates, Inc.},
 title = {Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/67d96d458abdef21792e6d8e590244e7-Paper.pdf},
 volume = {34},
 year = {2021}
}

@inproceedings{d3pm,
 author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {17981--17993},
 publisher = {Curran Associates, Inc.},
 title = {Structured Denoising Diffusion Models in Discrete State-Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
 volume = {34},
 year = {2021}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{ghazvininejad2019mask,
    title = "Mask-Predict: Parallel Decoding of Conditional Masked Language Models",
    author = "Ghazvininejad, Marjan  and
      Levy, Omer  and
      Liu, Yinhan  and
      Zettlemoyer, Luke",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1633",
    doi = "10.18653/v1/D19-1633",
    pages = "6112--6121",
    abstract = "Most machine translation systems generate text autoregressively from left to right. We, instead, use a masked language modeling objective to train a model to predict any subset of the target words, conditioned on both the input text and a partially masked target translation. This approach allows for efficient iterative decoding, where we first predict all of the target words non-autoregressively, and then repeatedly mask out and regenerate the subset of words that the model is least confident about. By applying this strategy for a constant number of iterations, our model improves state-of-the-art performance levels for non-autoregressive and parallel decoding translation models by over 4 BLEU on average. It is also able to reach within about 1 BLEU point of a typical left-to-right transformer model, while decoding significantly faster.",
}

@article{gu2019levenshtein,
  title={Levenshtein transformer},
  author={Gu, Jiatao and Wang, Changhan and Zhao, Junbo},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@inproceedings{stern2019insertion,
  title={Insertion transformer: Flexible sequence generation via insertion operations},
  author={Stern, Mitchell and Chan, William and Kiros, Jamie and Uszkoreit, Jakob},
  booktitle={International Conference on Machine Learning},
  pages={5976--5985},
  year={2019},
  organization={PMLR}
}
@inproceedings{qi2021bang,
  title={Bang: Bridging autoregressive and non-autoregressive generation with large scale pretraining},
  author={Qi, Weizhen and Gong, Yeyun and Jiao, Jian and Yan, Yu and Chen, Weizhu and Liu, Dayiheng and Tang, Kewen and Li, Houqiang and Chen, Jiusheng and Zhang, Ruofei and others},
  booktitle={International Conference on Machine Learning},
  pages={8630--8639},
  year={2021},
  organization={PMLR}
}
@inproceedings{kumar2004minimum,
  title={Minimum bayes-risk decoding for statistical machine translation},
  author={Kumar, Shankar and Byrne, Bill},
  booktitle={Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004},
  pages={169--176},
  year={2004}
}

@misc{stable_audio,
      title={Fast Timing-Conditioned Latent Audio Diffusion}, 
      author={Zach Evans and CJ Carr and Josiah Taylor and Scott H. Hawley and Jordi Pons},
      year={2024},
      eprint={2402.04825},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@misc{stable_video_diffusion,
      title={Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets}, 
      author={Andreas Blattmann and Tim Dockhorn and Sumith Kulal and Daniel Mendelevitch and Maciej Kilian and Dominik Lorenz and Yam Levi and Zion English and Vikram Voleti and Adam Letts and Varun Jampani and Robin Rombach},
      year={2023},
      eprint={2311.15127},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}
@misc{le2023bloom,
  title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},
  author={Le Scao, Teven and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  year={2023},
  eprint={2211.05100},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2211.05100}, 
}
@misc{bigscience,
      title={The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset}, 
      author={Hugo Laurençon and Lucile Saulnier and Thomas Wang and Christopher Akiki and Albert Villanova del Moral and Teven Le Scao and Leandro Von Werra and Chenghao Mou and Eduardo González Ponferrada and Huu Nguyen and Jörg Frohberg and Mario Šaško and Quentin Lhoest and Angelina McMillan-Major and Gerard Dupont and Stella Biderman and Anna Rogers and Loubna Ben allal and Francesco De Toni and Giada Pistilli and Olivier Nguyen and Somaieh Nikpoor and Maraim Masoud and Pierre Colombo and Javier de la Rosa and Paulo Villegas and Tristan Thrush and Shayne Longpre and Sebastian Nagel and Leon Weber and Manuel Muñoz and Jian Zhu and Daniel Van Strien and Zaid Alyafeai and Khalid Almubarak and Minh Chien Vu and Itziar Gonzalez-Dios and Aitor Soroa and Kyle Lo and Manan Dey and Pedro Ortiz Suarez and Aaron Gokaslan and Shamik Bose and David Adelani and Long Phan and Hieu Tran and Ian Yu and Suhas Pai and Jenny Chim and Violette Lepercq and Suzana Ilic and Margaret Mitchell and Sasha Alexandra Luccioni and Yacine Jernite},
      year={2023},
      eprint={2303.03915},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@inproceedings{wiki_auto,
  title={Neural CRF Model for Sentence Alignment in Text Simplification},
  author={Jiang, Chao and Maddela, Mounica and Lan, Wuwei and Zhong, Yang and Xu, Wei},
  booktitle={Proceedings of the Association for Computational Linguistics (ACL)},
  year={2020}
}

@inproceedings{bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}

@misc{roberta,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}
@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@inproceedings{mahabadi2023tess,
    title = "{TESS}: Text-to-Text Self-Conditioned Simplex Diffusion",
    author = "Karimi Mahabadi, Rabeeh  and
      Ivison, Hamish  and
      Tae, Jaesung  and
      Henderson, James  and
      Beltagy, Iz  and
      Peters, Matthew  and
      Cohan, Arman",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.eacl-long.144",
    pages = "2347--2361",
    abstract = "Diffusion models have emerged as a powerful paradigm for generation, obtaining strong performance in various continuous domains. However, applying continuous diffusion models to natural language remains challenging due to its discrete nature and the need for a large number of diffusion steps to generate text, making diffusion-based generation expensive.In this work, we propose Text-to-text Self-conditioned Simplex Diffusion (TESS), a text diffusion model that is fully non-autoregressive, employs a new form of self-conditioning, and applies the diffusion process on the logit simplex space rather than the learned embedding space.Through extensive experiments on natural language understanding and generation tasks including summarization, text simplification, paraphrase generation, and question generation, we demonstrate that TESS outperforms state-of-the-art non-autoregressive models, requires fewer diffusion steps with minimal drop in performance, and is competitive with pretrained autoregressive sequence-to-sequence models.",
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% GAN %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Classic %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% SOTA %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{karras2021alias,
  title={Alias-free generative adversarial networks},
  author={Karras, Tero and Aittala, Miika and Laine, Samuli and H{\"a}rk{\"o}nen, Erik and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% Problems %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{arjovsky2017wasserstein,
  title={Wasserstein gan. arXiv 2017},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  volume={30},
  pages={4},
  year={2017}
}
@article{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{karras2019style,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}
@article{brock2018large,
  title={Large scale GAN training for high fidelity natural image synthesis},
  author={Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  journal={arXiv preprint arXiv:1809.11096},
  year={2018}
}
@article{zhao2018bias,
  title={Bias and generalization in deep generative models: An empirical study},
  author={Zhao, Shengjia and Ren, Hongyu and Yuan, Arianna and Song, Jiaming and Goodman, Noah and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{thanh2020catastrophic,
  title={Catastrophic forgetting and mode collapse in GANs},
  author={Thanh-Tung, Hoang and Tran, Truyen},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--10},
  year={2020},
  organization={IEEE}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% AR %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% SOTA %%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

%%%%%%%%%%%%%%%%%%%%%%%%%% Problems %%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{zhang2021understanding,
  title={Understanding failures in out-of-distribution detection with deep generative models},
  author={Zhang, Lily and Goldstein, Mark and Ranganath, Rajesh},
  booktitle={International Conference on Machine Learning},
  pages={12427--12436},
  year={2021},
  organization={PMLR}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% VAE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Classic %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{rezende2014stochastic,
  title={Stochastic backpropagation and approximate inference in deep generative models},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  booktitle={International conference on machine learning},
  pages={1278--1286},
  year={2014},
  organization={PMLR}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Flow %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{grathwohl2018ffjord,
  title={Ffjord: Free-form continuous dynamics for scalable reversible generative models},
  author={Grathwohl, Will and Chen, Ricky TQ and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  journal={arXiv preprint arXiv:1810.01367},
  year={2018}
}
@article{chen2019residual,
  title={Residual flows for invertible generative modeling},
  author={Chen, Ricky TQ and Behrmann, Jens and Duvenaud, David K and Jacobsen, J{\"o}rn-Henrik},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% EBM %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{xiao2020vaebm,
  title={Vaebm: A symbiosis between variational autoencoders and energy-based models},
  author={Xiao, Zhisheng and Kreis, Karsten and Kautz, Jan and Vahdat, Arash},
  journal={arXiv preprint arXiv:2010.00654},
  year={2020}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Diffusion %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%% Classic %%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{sohl2015deep,
  title={Deep unsupervised learning using nonequilibrium thermodynamics},
  author={Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={International Conference on Machine Learning},
  pages={2256--2265},
  year={2015},
  organization={PMLR}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% DDPM %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2006.11239},
  year={2020}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% Song + Ermon %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={arXiv preprint arXiv:1907.05600},
  year={2019}
}
@article{song2020improved,
  title={Improved techniques for training score-based generative models},
  author={Song, Yang and Ermon, Stefano},
  journal={arXiv preprint arXiv:2006.09011},
  year={2020}
}
@article{song2020denoising,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2010.02502},
  year={2020}
}
@article{song2020score,
  title={Score-based generative modeling through stochastic differential equations},
  author={Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  journal={arXiv preprint arXiv:2011.13456},
  year={2020}
}

@article{kawar2022denoising,
  title={Denoising diffusion restoration models},
  author={Kawar, Bahjat and Elad, Michael and Ermon, Stefano and Song, Jiaming},
  journal={arXiv preprint arXiv:2201.11793},
  year={2022}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% OpenAI %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{nichol2021improved,
  title={Improved denoising diffusion probabilistic models},
  author={Nichol, Alex and Dhariwal, Prafulla},
  journal={arXiv preprint arXiv:2102.09672},
  year={2021}
}
@article{dhariwal2021diffusion,
  title={Diffusion models beat gans on image synthesis},
  author={Dhariwal, Prafulla and Nichol, Alex},
  journal={arXiv preprint arXiv:2105.05233},
  year={2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% Variational diffusion models %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{kingma2021variational,
  title={Variational diffusion models},
  author={Kingma, Diederik P and Salimans, Tim and Poole, Ben and Ho, Jonathan},
  journal={arXiv preprint arXiv:2107.00630},
  volume={2},
  year={2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% Speed up %%%%%%%%%%%%%%%%%%%%%%%%%%
@article{tachibana2021taylor,
  title={It$\backslash$\^{}$\{$o$\}$-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives},
  author={Tachibana, Hideyuki and Go, Mocho and Inahara, Muneyoshi and Katayama, Yotaro and Watanabe, Yotaro},
  journal={arXiv preprint arXiv:2112.13339},
  year={2021}
}
@article{liu2022pseudo,
  title={Pseudo numerical methods for diffusion models on manifolds},
  author={Liu, Luping and Ren, Yi and Lin, Zhijie and Zhao, Zhou},
  journal={arXiv preprint arXiv:2202.09778},
  year={2022}
}
@article{xiao2021tackling,
  title={Tackling the Generative Learning Trilemma with Denoising Diffusion GANs},
  author={Xiao, Zhisheng and Kreis, Karsten and Vahdat, Arash},
  journal={arXiv preprint arXiv:2112.07804},
  year={2021}
}
@article{salimans2022progressive,
  title={Progressive distillation for fast sampling of diffusion models},
  author={Salimans, Tim and Ho, Jonathan},
  journal={arXiv preprint arXiv:2202.00512},
  year={2022}
}
@article{vahdat2021score,
  title={Score-based generative modeling in latent space},
  author={Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{zhang2021diffusion,
  title={Diffusion Normalizing Flow},
  author={Zhang, Qinsheng and Chen, Yongxin},
  journal={arXiv preprint arXiv:2110.07579},
  year={2021}
}


%%%%%%%%%%%%%%%%%% SR3 %%%%%%%%%%%%%%%%%%
@article{saharia2021image,
  title={Image super-resolution via iterative refinement},
  author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J and Norouzi, Mohammad},
  journal={arXiv preprint arXiv:2104.07636},
  year={2021}
}


%%%%%%%%%%%%%%%%%% Sound + shape %%%%%%%%%%%%%%%%%%
@inproceedings{popov2021grad,
  title={Grad-tts: A diffusion probabilistic model for text-to-speech},
  author={Popov, Vadim and Vovk, Ivan and Gogoryan, Vladimir and Sadekova, Tasnima and Kudinov, Mikhail},
  booktitle={International Conference on Machine Learning},
  pages={8599--8608},
  year={2021},
  organization={PMLR}
}
@article{liu2022diffgan,
  title={DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs},
  author={Liu, Songxiang and Su, Dan and Yu, Dong},
  journal={arXiv preprint arXiv:2201.11972},
  year={2022}
}
@inproceedings{luo2021diffusion,
  title={Diffusion probabilistic models for 3d point cloud generation},
  author={Luo, Shitong and Hu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2837--2845},
  year={2021}
}
@inproceedings{zhou20213d,
  title={3d shape generation and completion through point-voxel diffusion},
  author={Zhou, Linqi and Du, Yilun and Wu, Jiajun},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5826--5835},
  year={2021}
}


%%%%%%%%%%%%%%%%%%%%%%%%%% Text's diffusion %%%%%%%%%%%%%%%%%%%%%%%%%%

@article{austin2021structured,
  title={Structured denoising diffusion models in discrete state-spaces},
  author={Austin, Jacob and Johnson, Daniel D and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17981--17993},
  year={2021}
}



%%%%%%%%%%%%%%%%%%%%%%%%%% NLP %%%%%%%%%%%%%%%%%%%%%%%%%%

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{dodge2021documenting,
  title={Documenting large webtext corpora: A case study on the colossal clean crawled corpus},
  author={Dodge, Jesse and Sap, Maarten and Marasovi{\'c}, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Mitchell, Margaret and Gardner, Matt},
  journal={arXiv preprint arXiv:2104.08758},
  year={2021}
}


@article{bloom2022hug,
  title={Bigscience large open-science openaccess multilingual language model},
  author={BigScience},
  journal={https://huggingface.co/bigscience/bloom},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{paradetox,
author = {Logacheva, Varvara and Dementieva, Daryna and Ustyantsev, Sergey and Moskovskiy, Daniil and Dale, David and Krotova, Irina and Semenov, Nikita and Panchenko, Alexander},
year = {2022},
month = {01},
pages = {6804-6818},
title = {ParaDetox: Detoxification with Parallel Data},
doi = {10.18653/v1/2022.acl-long.469}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{chen2022analog,
  title={Analog bits: Generating discrete data using diffusion models with self-conditioning},
  author={Chen, Ting and Zhang, Ruixiang and Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2208.04202},
  year={2022}
}

@inproceedings{bao2023all,
  title={All are worth words: A vit backbone for diffusion models},
  author={Bao, Fan and Nie, Shen and Xue, Kaiwen and Cao, Yue and Li, Chongxuan and Su, Hang and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22669--22679},
  year={2023}
}

@misc{theoretical_repetitions,
      title={A Theoretical Analysis of the Repetition Problem in Text Generation}, 
      author={Zihao Fu and Wai Lam and Anthony Man-Cho So and Bei Shi},
      year={2021},
      eprint={2012.14660},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{d3pm,
  author       = {Jacob Austin and
                  Daniel D. Johnson and
                  Jonathan Ho and
                  Daniel Tarlow and
                  Rianne van den Berg},
  title        = {Structured Denoising Diffusion Models in Discrete State-Spaces},
  journal      = {CoRR},
  volume       = {abs/2107.03006},
  year         = {2021},
  url          = {https://arxiv.org/abs/2107.03006},
  eprinttype    = {arXiv},
  eprint       = {2107.03006},
  timestamp    = {Mon, 25 Oct 2021 07:55:50 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2107-03006.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{mtd,
      title={Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions}, 
      author={Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forré and Max Welling},
      year={2021},
      eprint={2102.05379},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2102.05379}, 
}

@misc{sedd,
      title={Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution}, 
      author={Aaron Lou and Chenlin Meng and Stefano Ermon},
      year={2024},
      eprint={2310.16834},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2310.16834}, 
}

@misc{diffusionbert,
      title={DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models}, 
      author={Zhengfu He and Tianxiang Sun and Kuanning Wang and Xuanjing Huang and Xipeng Qiu},
      year={2022},
      eprint={2211.15029},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15029}, 
}

@misc{diffusionnat,
      title={Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation}, 
      author={Kun Zhou and Yifan Li and Wayne Xin Zhao and Ji-Rong Wen},
      year={2023},
      eprint={2305.04044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.04044}, 
}

@misc{mdlm,
      title={Simple and Effective Masked Diffusion Language Models}, 
      author={Subham Sekhar Sahoo and Marianne Arriola and Yair Schiff and Aaron Gokaslan and Edgar Marroquin and Justin T Chiu and Alexander Rush and Volodymyr Kuleshov},
      year={2024},
      eprint={2406.07524},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.07524}, 
}

@misc{concretescorematching,
      title={Concrete Score Matching: Generalized Score Matching for Discrete Data}, 
      author={Chenlin Meng and Kristy Choi and Jiaming Song and Stefano Ermon},
      year={2023},
      eprint={2211.00802},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.00802}, 
}


%%%%%%%%%%%%%%%%%%%%%%%%%% AR-Diffusion Links %%%%%%%%%%%%%%%%%%%%%%%%%%

@misc{wu2023ardiffusionautoregressivediffusionmodel,
      title={AR-Diffusion: Auto-Regressive Diffusion Model for Text Generation}, 
      author={Tong Wu and Zhihao Fan and Xiao Liu and Yeyun Gong and Yelong Shen and Jian Jiao and Hai-Tao Zheng and Juntao Li and Zhongyu Wei and Jian Guo and Nan Duan and Weizhu Chen},
      year={2023},
      eprint={2305.09515},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.09515}, 
}

@article{li2022diffusion,
  title={Diffusion-LM improves controllable text generation},
  author={Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2305.09515},
  year={2022},
}

@inproceedings{yuan2022seqdiffseq,
  title={SeqDiffSeq: Text diffusion with encoder-decoder transformers},
  author={Yuan, Hongyi and Yuan, Zhengyuan and Tan, Chuanqi and Huang, Fei and Huang, Songfang},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2022},
}

@inproceedings{lin2023genie,
  title={Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise},
  author={Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Duan, Nan and Chen, Weizhu},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023},
}



@misc{han2023ssdlmsemiautoregressivesimplexbaseddiffusion,
      title={SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control}, 
      author={Xiaochuang Han and Sachin Kumar and Yulia Tsvetkov},
      year={2023},
      eprint={2210.17432},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.17432}, 
}

@misc{mahabadi2024tesstexttotextselfconditionedsimplex,
      title={TESS: Text-to-Text Self-Conditioned Simplex Diffusion}, 
      author={Rabeeh Karimi Mahabadi and Hamish Ivison and Jaesung Tae and James Henderson and Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2024},
      eprint={2305.08379},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.08379}, 
}


%%%%%%%%%%%%%%%%%%% Scaling %%%%%%%%%%%%%%%%%%%%%%%%

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{kashyap2023gptneocommonsensereasoning,
      title={GPT-Neo for commonsense reasoning -- a theoretical and practical lens}, 
      author={Rohan Kashyap and Vivek Kashyap and Narendra C. P.},
      year={2023},
      eprint={2211.15593},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15593}, 
}

@misc{simplifiedmaskeddiffusion,
      title={Simplified and Generalized Masked Diffusion for Discrete Data}, 
      author={Jiaxin Shi and Kehang Han and Zhe Wang and Arnaud Doucet and Michalis K. Titsias},
      year={2024},
      eprint={2406.04329},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04329}, 
}

@misc{diffusionlm,
      title={Diffusion-LM Improves Controllable Text Generation}, 
      author={Xiang Lisa Li and John Thickstun and Ishaan Gulrajani and Percy Liang and Tatsunori B. Hashimoto},
      year={2022},
      eprint={2205.14217},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.14217}, 
}

@misc{diffuseq,
      title={DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models}, 
      author={Shansan Gong and Mukai Li and Jiangtao Feng and Zhiyong Wu and Lingpeng Kong},
      year={2023},
      eprint={2210.08933},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.08933}, 
}

@inproceedings{difformer,
    title = "Empowering Diffusion Models on the Embedding Space for Text Generation",
    author = "Gao, Zhujin  and
      Guo, Junliang  and
      Tan, Xu  and
      Zhu, Yongxin  and
      Zhang, Fang  and
      Bian, Jiang  and
      Xu, Linli",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.261",
    doi = "10.18653/v1/2024.naacl-long.261",
    pages = "4664--4683",
}

@misc{planner,
      title={PLANNER: Generating Diversified Paragraph via Latent Language Diffusion Model}, 
      author={Yizhe Zhang and Jiatao Gu and Zhuofeng Wu and Shuangfei Zhai and Josh Susskind and Navdeep Jaitly},
      year={2024},
      eprint={2306.02531},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.02531}, 
}

@misc{dinoiser,
      title={DINOISER: Diffused Conditional Sequence Learning by Manipulating Noises}, 
      author={Jiasheng Ye and Zaixiang Zheng and Yu Bao and Lihua Qian and Mingxuan Wang},
      year={2024},
      eprint={2302.10025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.10025}, 
}

@misc{analogbits,
      title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning}, 
      author={Ting Chen and Ruixiang Zhang and Geoffrey Hinton},
      year={2023},
      eprint={2208.04202},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2208.04202}, 
}

@misc{sed,
      title={Self-conditioned Embedding Diffusion for Text Generation}, 
      author={Robin Strudel and Corentin Tallec and Florent Altché and Yilun Du and Yaroslav Ganin and Arthur Mensch and Will Grathwohl and Nikolay Savinov and Sander Dieleman and Laurent Sifre and Rémi Leblond},
      year={2022},
      eprint={2211.04236},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.04236}, 
}

@inproceedings{ld4lg,
 author = {Lovelace, Justin and Kishore, Varsha and Wan, Chao and Shekhtman, Eliot and Weinberger, Kilian Q},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {56998--57025},
 publisher = {Curran Associates, Inc.},
 title = {Latent Diffusion for Language Generation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/b2a2bd5d5051ff6af52e1ef60aefd255-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

@misc{flamingo,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2204.14198}, 
}

@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}